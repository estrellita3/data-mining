{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización\n",
    "\n",
    "Consiste en encontrar en mínimo o máximo de una función. Para todos los ejemplos vamos a suponer que buscamos el mínimo de una función.\n",
    "\n",
    "Dada una función $f(X)$ donde $X$ puede ser una constante, un vector o una matriz, el objetivo es encontrar un valor de X tal que se garantice \n",
    "\n",
    "\\\\[\n",
    "x^* = {argmin} _{x\\in \\mathbb {R} } \\left\\{ f(x) \\right\\}\n",
    "\\\\]\n",
    "\n",
    "Por ejemplo para $f(x)=x^2+3$:\n",
    "\n",
    "\\\\[\n",
    "x^* = {argmin} _{x\\in \\mathbb {R} } \\left\\{ x^2 \\right\\} = 0\n",
    "\\\\]\n",
    "\n",
    "\\\\[\n",
    "f(x^*) = f(0) = 0^2+3 = 3\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=4,repr.plot.width=6,repr.plot.res = 300)\n",
    "\n",
    "myfunc<-function(x){ x^2+3 }\n",
    "\n",
    "x<-seq(-2,2,length.out = 100)\n",
    "y<-myfunc(x)\n",
    "\n",
    "plot(x,y,t=\"l\")\n",
    "points(0,3,col=\"red\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué queremos optimizar?\n",
    "\n",
    "Todos los algoritmos que usemos van a tener una *función de error* que va a medir como se ajusta nuestra curva de *predicción* a la curva *real*.\n",
    "\n",
    "Es lo que en inglés se conoce como [curve-fitting].(https://en.wikipedia.org/wiki/Curve_fitting)\n",
    "\n",
    "![](./img/CajaNegra.png)\n",
    "\n",
    "El objetivo de esa función de error es saber cuanto se equivoca el algoritmo en su decisión. \n",
    "\n",
    "Siempre vamos a tratar de minimizar el valor de esa función de error cambiando los parámetros necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo: \n",
    "\n",
    "Regresión lineal:\n",
    "\\\\[\n",
    "y= a·x +b    \n",
    "\\\\]\n",
    "\n",
    "Dados unos valores de ($x_{real}$,$y_{real}$) conocidos tenemos que que ver que valores de (a,b) miniminzan:\n",
    "\\\\[\n",
    "f_{mse}(a,b)=\\sqrt{ \\sum{(a·x_{real} +b-y_{real} )^2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los problemas *aprendizaje* son formas más o menos avanzadas de ajuste de curvas:\n",
    "\n",
    "https://diginomica.com/ai-curve-fitting-not-intelligence\n",
    "\n",
    "\n",
    "*As much as I look into what’s being done with deep learning, I see they’re all stuck there on the level of associations. Curve fitting. That sounds like sacrilege, to say that all the impressive achievements of deep learning amount to just fitting a curve to data. From the point of view of the mathematical hierarchy, no matter how skillfully you manipulate the data and what you read into the data when you manipulate it, it’s still a curve-fitting exercise, albeit complex and nontrivial.* [Judea Pearl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo encontramos el mínimo de una función?\n",
    "\n",
    "Podemos calcularlo de forma **iterativa**. Es decir, inyectando en nuestro modelo un valor distinto para ver que valor tenemos a la salida. En función de los datos que metemos en la entrada vemos cual es la salida que tiene un valor más bajo.\n",
    "\n",
    "Al ser un proceso iterativo tenemos que garantizar convergencia, es decir, dado un número suficientemente grande de iteraciones nuestro proceso encontrará el mínimo.\n",
    "\n",
    "En algunos casos muy contados podemos saltar diréctamente al mínimo, pero por lo general este no será el caso.\n",
    "\n",
    "### Ejemplo 1:\n",
    "\n",
    "```\n",
    "1. x_0 <- Asignamos un valor x al azar\n",
    "2. x_new <- Elegimos otro valor de x próximo a x_0  al azar\n",
    "3. Si f(x_new)<f(x_0) entonces x_0 <- x_new\n",
    "4. Repetimos desde 2 un número N de veces\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 <- runif(1,-2,2)\n",
    "hiperparametro <- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x,y,t=\"l\")\n",
    "grid()\n",
    "\n",
    "x_new <- rnorm(1,mean=x_0,sd= hiperparametro )\n",
    "if (myfunc(x_new) < myfunc(x_0)){\n",
    "    x_0 <- x_new\n",
    "}\n",
    "points(x_new,myfunc(x_new),col=\"blue\")\n",
    "points(x_0,myfunc(x_0),col=\"red\",pch='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 <- runif(1,-10,10)\n",
    "hiperparametro <- 2\n",
    "\n",
    "x_historico <- c()\n",
    "for (i in 1:20){\n",
    "    x_new <- rnorm(1,mean=x_0,sd= hiperparametro )\n",
    "    if (myfunc(x_new) < myfunc(x_0)){\n",
    "        x_0 <- x_new\n",
    "    }\n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "plot(x_historico,t=\"o\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2:\n",
    "\n",
    "```\n",
    "1. x_0 <- Asignamos un valor x al azar\n",
    "2. x_new1 <- x_0 + delta\n",
    "3. x_new2 <- x_0 - delta\n",
    "4. Si f(x_new1)<f(x_0) entonces x_0 <- x_new1 \n",
    "      sino  si f(x_new2)<f(x_0) entonces x_0 <- x_new2\n",
    "      sino FIN, el minimo esta cerca de x_0\n",
    "5. Repetimos desde 2 un número N de veces\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 <- runif(1,-3,3)\n",
    "hiperparametro <- 2\n",
    "delta <- 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x,y,t=\"l\")\n",
    "grid()\n",
    "\n",
    "x_new1 <- x_0 + delta\n",
    "x_new2 <- x_0 - delta\n",
    "\n",
    "if (myfunc(x_new1) < myfunc(x_0)){\n",
    "    x_0 <- x_new1\n",
    "}else if (myfunc(x_new2) < myfunc(x_0)){\n",
    "    x_0 <- x_new2\n",
    "}\n",
    "\n",
    "points(x_0,myfunc(x_0),col=\"red\",pch='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 <- runif(1,-10,10)\n",
    "delta <- 0.3\n",
    "x_historico <- c()\n",
    "for (i in 1:100){\n",
    "    x_new1 <- x_0 + delta\n",
    "    x_new2 <- x_0 - delta\n",
    "    \n",
    "    if (myfunc(x_new1) < myfunc(x_0)){\n",
    "        x_0 <- x_new1\n",
    "    }else if (myfunc(x_new2) < myfunc(x_0)){\n",
    "        x_0 <- x_new2\n",
    "    }else{\n",
    "        break;\n",
    "    }\n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "plot(x_historico,t=\"o\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada\n",
    "\n",
    "La derivada de una función es su pendiente:\n",
    "\\\\[\n",
    "    f'(x)=\\frac{\\partial f(x)}{\\partial x} = \\lim\\limits_{h \\to 0}\\frac{f(x+h)-f(x)}{h}\n",
    "\\\\]\n",
    "En nuestra función $f(x)=x^2+3$ la derivada sería:\n",
    "\n",
    "\\\\[\n",
    "    f'(x) = \\lim\\limits_{h \\to 0}\\frac{(x+h)^2 + 3 - x^2 - 3}{h}\n",
    "\\\\]\n",
    "\n",
    "\\\\[\n",
    "    f'(x) = \\lim\\limits_{h \\to 0}\\frac{ x^2 + h^2 + 2xh - x^2}{h}\n",
    "\\\\]\n",
    "\n",
    "\\\\[\n",
    "    f'(x) = \\lim\\limits_{h \\to 0}\\frac{  h^2 + 2xh}{h} = 2x\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cómo de pequeño tiene que ser h?\n",
    "x <- 5\n",
    "for (h in c(1,0.5,0.1,0.01,1e-3,1e-4,1e-5)){\n",
    "    print((myfunc(x+h)-myfunc(x))/h)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pintar la derivada junto con la función\n",
    "\n",
    "La derivada es $f'(x)=2x$, lo que hacemos es pintar una recta $y = a·x+b$.\n",
    "Sabemos que la pendiente es $f'(x)=a=2x$. Lo centramos en $f(x)$, así que $b=f(x)$ y dibujamos la recta desde $[x-delta,x+delta]$ para:\n",
    "* $x = -7$ : $f'(x)=-14$ y   $f(x)=52$\n",
    "* $x = 0$ : $f'(x)=0$ y   $f(x)=3$\n",
    "* $x = 5$ : $f'(x)=5$ y   $f(x)=28$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-seq(-10,10,length.out = 100)\n",
    "y<-myfunc(x)\n",
    "\n",
    "plot(x,y,t=\"l\")\n",
    "points(0,3,col=\"red\")\n",
    "grid()\n",
    "\n",
    "plotDerivada<-function(x){\n",
    "    delta <- 2  \n",
    "    lines(c(x-delta,x+delta),\n",
    "          c(myfunc(x)-delta*2*x,myfunc(x)+delta*2*x),\n",
    "          col=\"red\")\n",
    "}\n",
    "\n",
    "plotDerivada(5)\n",
    "plotDerivada(-7)\n",
    "#plotDerivada(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada tiene un valor:\n",
    "* $f'(x)<0$: **negativo** cuando la función disminuye a medida que aumenta x\n",
    "* $f'(x)>0$: **positivo** cuando la función aumenta a medida que aumenta x\n",
    "* $f'(x)=0$: cuando estamos en un máximo, **mínimo** o punto de inflexión\n",
    "\n",
    "Cuanto mayor es el **valor absoluto** de la derivada, mayor es la pendiente de la función, más rápidamente cambia la función.\n",
    "\n",
    "**¿Como podemos diferenciar entre máximo, mínimo y punto de inflexión?**\n",
    "\n",
    "La **segunda derivada**, es decir, la derivada de la derivada nos da información de cómo cambia la derivada. \n",
    "\\\\[\n",
    "f'' \\left(x \\right) = \\frac{\\partial }{\\partial x}f' \\left(x \\right) =\\frac{\\partial }{\\partial x \\partial x}f \\left(x \\right) \n",
    "\\\\]\n",
    "\n",
    "* $f''(x) > 0$ : Indica que $f'(x)$ va a aumentar al incrementar $x$.  Si $f'(x)=0$ nos encontramos en un mínimo.\n",
    "* $f''(x) < 0$ : Indica que $f'(x)$ va a disminuir al incrementar $x$. Si $f'(x)=0$ nos encontramos en un máximo.\n",
    "* $f''(x) = 0$ : Indica que $f'(x)$ va a seguir igual al incrementar $x$. Si $f'(x)=0$ nos encontramos en un punto de inflexión.\n",
    "\n",
    "La segunda derivada es un elemento que podemos usar para saber si estamos en un máximo, un mínimo o un punto de inflexión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos modificar el ejemplo 2 utilizando lo que hemos aprendido de derivadas:\n",
    "```\n",
    "1. x_0 <- Asignamos un valor x al azar\n",
    "2. x_new <- x_0 - f'(x_0)*delta\n",
    "3. Repetimos desde 2 un número N de veces\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivada_myfunc<-function(x){ 2*x }\n",
    "\n",
    "x_0 <- 7\n",
    "delta <- 0.1\n",
    "x_historico<-c()\n",
    "for (i in 1:10){\n",
    "    x_new <- x_0 - derivada_myfunc(x_0)*delta\n",
    "    x_0 <- x_new\n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "plot(x_historico,t=\"o\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema: **\n",
    "\n",
    "Muchas veces no tenemos una función conocida, solo una *caja negra*. ¿Cómo podemos calcular su derivada?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivada_myfunc<-function(x,h,func){ (func(x+h)-func(x))/h }\n",
    "\n",
    "x_0 <- 7\n",
    "delta <- 0.8\n",
    "h <- 0.001\n",
    "x_historico<-c()\n",
    "for (i in 1:50){\n",
    "    x_new <- x_0 - derivada_myfunc(x_0,h,myfunc)*delta    \n",
    "    x_0 <- x_new    \n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "plot(x_historico,t=\"o\")\n",
    "grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterio de convergencia\n",
    "\n",
    "Hasta ahora habíamos ejecutado nuestro algoritmo un número N de iteraciones pero deberíamos parar cuando la función converge al mínimo. A la hora de definir el criterio de convergencia hay que tener en cuenta:\n",
    "* Si es demasiado restrictivo es posible que nunca se llegue.\n",
    "* Si es demasiado laxo no conseguiremos optimizar todo lo posible.\n",
    "\n",
    "Existen varias formas de definirlo:\n",
    "* Si tras M iteraciones no se consigue disminuir el valor de la función a optimizar.\n",
    "* Si la derivada es practicamente 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 <- 7\n",
    "delta <- 0.8\n",
    "h <- 0.001\n",
    "criterio_convergencia<-0.05\n",
    "x_historico<-c()\n",
    "\n",
    "for (i in 1:50){\n",
    "    x_new <- x_0 - derivada_myfunc(x_0,h,myfunc)*delta    \n",
    "    if (abs(myfunc(x_new)-myfunc(x_0))<criterio_convergencia){\n",
    "        x<-ifelse(myfunc(x_new) < myfunc(x_0),myfunc(x_new),myfunc(x_0))\n",
    "        break;\n",
    "    }\n",
    "    x_0 <- x_new\n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "plot(x_historico,t=\"o\")\n",
    "grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mínimo local vs Minimo global\n",
    "\n",
    "Las funciones que trataremos de minimizar muchas vecse no serán funciones convexas como la que hemos visto,  podrán tener varios mínimos locales. \n",
    "\n",
    "Es imposible garantizar que nuestro algoritmo de optimización acabará en un mínimo global, pero por lo menos nos tenemos que asegurar que cae en un mínimo local con un valor lo suficientemente cercano al del mínimo global.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc2<-function(x){0.05*x^2+sin(x)}\n",
    "x<-seq(-10,10,length.out = 100)\n",
    "y<-myfunc2(x)\n",
    "plot(x,y,t=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(plotrix)\n",
    "derivada_myfunc<-function(x,h,func){ (func(x+h)-func(x))/h }\n",
    "\n",
    "\n",
    "x_0 <- -20\n",
    "delta <- 1\n",
    "#delta <- 3 #Cambiar delta\n",
    "h <- 0.001\n",
    "criterio_convergencia<-0.5\n",
    "x_historico<-c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new <- x_0 - derivada_myfunc(x_0,h,myfunc2)*delta    \n",
    "x_0 <- x_new\n",
    "x_0\n",
    "x_historico<-c(x_historico,x_0)\n",
    "\n",
    "\n",
    "plot(x,y,t=\"l\")\n",
    "points(x_historico,myfunc2(x_historico),col=\"red\")\n",
    "color.scale.lines(x_historico,myfunc2(x_historico),c(1,1,0),0,c(0,1,1),colvar=1:length(x_historico),lwd=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(plotrix)\n",
    "derivada_myfunc<-function(x,h,func){ (func(x+h)-func(x))/h }\n",
    "\n",
    "\n",
    "x_0 <- -20\n",
    "delta <- 1\n",
    "#delta <- 3 #Cambiar delta\n",
    "h <- 0.001\n",
    "criterio_convergencia<-0.5\n",
    "\n",
    "x_historico<-c()\n",
    "for (i in 1:50){\n",
    "    x_new <- x_0 - derivada_myfunc(x_0,h,myfunc2)*delta    \n",
    "    if (abs(myfunc(x_new)-myfunc(x_0))<criterio_convergencia){\n",
    "        x_0<-ifelse(myfunc(x_new) < myfunc(x_0),myfunc(x_new),myfunc(x_0))\n",
    "        break;\n",
    "    }\n",
    "    x_0 <- x_new\n",
    "    x_historico<-c(x_historico,x_0)\n",
    "}\n",
    "\n",
    "plot(x,y,t=\"l\")\n",
    "points(x_historico,myfunc2(x_historico),col=\"red\")\n",
    "color.scale.lines(x_historico,myfunc2(x_historico),c(1,1,0),0,c(0,1,1),colvar=1:length(x_historico),lwd=2)\n",
    "\n",
    "plot(x_historico,t=\"o\")\n",
    "color.scale.lines(1:length(x_historico),x_historico,c(1,1,0),0,c(0,1,1),colvar=1:length(x_historico),lwd=2)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio vectorial\n",
    "\n",
    "La mayor parte de las funciones a optimizar no toman como entrada no un escalar sino un vector. En este caso el vector definirá un expacio en el cual habrá que encontrar el valor que produzca el mínimo valor.\n",
    "\n",
    "Hasta ahora hemos visto la derivada de una función. Ahora vamos a introducir el concepto de derivada parcial:\n",
    "\n",
    "Sea una función $f: \\mathbb {R}^n \\rightarrow  \\mathbb {R}$ la derivada parcial $\\frac{\\partial }{\\partial x_i}f \\left(\\vec{x} \\right)$ mide como cambia $f \\left(\\vec{x} \\right)$ en función de la variable $x_i$, esto es la columna *i-ésima* del vector $\\vec{x}$.\n",
    "\n",
    "Por ejemplo:\n",
    "\\\\[\n",
    "f(x_1,x_2)=x_1^2+3*x_2^2\n",
    "\\\\]\n",
    "\n",
    "La derivada parcial respecto a $x_1$ es: $\\frac{\\partial }{\\partial x_1}f \\left(\\vec{x} \\right) = 2x_1$\n",
    "\n",
    "La derivada parcial respecto a $x_2$ es: $\\frac{\\partial }{\\partial x_2}f \\left(\\vec{x} \\right) = 6x_2$\n",
    "\n",
    "\n",
    "El **gradiente** es la derivada respecto al vector $\\vec{x}$, es una generalización multivariable de la derivada. Se denota con el símbolo $\\nabla f$. El resultado es un vector, a diferencia de la derivada cuyo resultado es un escalar.\n",
    "\\\\[\n",
    "\\nabla f(x_1,x_2)=2x_1·\\vec{u}+6x_2·\\vec{v}\n",
    "\\\\]\n",
    "Donde $\\vec{u}$ y $\\vec{v}$ son los vectores unitarios de las coordenadas dadas por $x_1$ y $x_2$ respectivamente.\n",
    "\n",
    "El gradiente indica la dirección de máximo crecimiento de la función, y su valor negativo indica la dirección de máximo de decrecimiento.\n",
    "\n",
    "Así pues el **método de descenso de gradientes** se basa en encontrar la dirección de máximo decrecimiento de la función y actualizar el valor de $\\vec{x}$ en conformidad.\n",
    "\\\\[\n",
    "\\vec{x}_{new} = \\vec{x}-\\epsilon \\nabla f \\left(\\vec{x} \\right)\n",
    "\\\\]\n",
    "\n",
    "Donde $\\epsilon$ es una constante, un hiperparámetro, que hay que definir al principio de la iteración. Se le conoce como **tasa de aprendizaje** (learning rate). Un valor pequeño puede hacer que se necesiten muchas iteraciones para converger, una valor grande puede hacernos perder el máximo de la función.\n",
    "\n",
    "El método de descenso de gradientes converge cuando todos los elemenos del gradiente son cero (o muy próximos a cero).\n",
    "\n",
    "El método de descenso de gradientes se considera un algoritmo de **primer orden** porque utiliza información solo de la primera derivada. Nunca utiliza información de la segunda derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfuncVector<-function(x){ x[1]^2+3*x[2]^2 }\n",
    "\n",
    "mygradient<-function(x,h,func){ \n",
    "    c((func(c(x[1]+h,x[2]))-func(x))/h,\n",
    "      (func(c(x[1],x[2]+h))-func(x))/h) }\n",
    "\n",
    "\n",
    "mygradient(c(2,2),0.001,myfuncVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(plotrix)\n",
    "\n",
    "\n",
    "x_0 <- c(-9,-9)\n",
    "delta <- 0.15\n",
    "h <- 0.001\n",
    "criterio_convergencia<-0.05\n",
    "\n",
    "x_historico<-data.frame(x=x_0[1],y=x_0[2])\n",
    "for (i in 1:100){\n",
    "    g <- mygradient(x_0,h,myfuncVector)\n",
    "    if(abs(sum(g))<criterio_convergencia){\n",
    "        break;\n",
    "    }\n",
    "    x_new <- x_0 - g*delta     \n",
    "    x_0 <- x_new\n",
    "    x_historico<-rbind(x_historico,data.frame(x=x_0[1],y=x_0[2]))\n",
    "}\n",
    "\n",
    "myfunc3D<-function(x1,x2){x1^2+3*x2^2}\n",
    "x <- seq(-10,10,length=100)\n",
    "y <- seq(-10,10,length=100)\n",
    "z <- outer(x,y,myfunc3D)\n",
    "image(x,y,z,col = terrain.colors(39))\n",
    "contour(x,y,z,add=T)\n",
    "points(x_historico$x,x_historico$y,col=\"black\")\n",
    "color.scale.lines(x_historico$x,x_historico$y,c(1,1,0),0,c(0,1,1),colvar=1:nrow(x_historico),lwd=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc3D<-function(x1,x2){sin(x1)*sin(x2)/(x1*x2)}\n",
    "x <- seq(-10,10,length=100)\n",
    "y <- seq(-10,10,length=100)\n",
    "z <- outer(x,y,myfunc3D)\n",
    "image(x,y,z,col = terrain.colors(39))\n",
    "contour(x,y,z,add=T)\n",
    "\n",
    "myfuncVector<-function(x){ myfunc3D(x[1],x[2]) }\n",
    "\n",
    "mygradient<-function(x,h,func){ \n",
    "    c((func(c(x[1]+h,x[2]))-func(x))/h,\n",
    "      (func(c(x[1],x[2]+h))-func(x))/h) }\n",
    "\n",
    "#x_0 <- c(-2,-7)\n",
    "x_0 <- c(-5,-4)\n",
    "#x_0 <- c(0.7,1)\n",
    "delta <- 5\n",
    "h <- 0.001\n",
    "criterio_convergencia<-1e-3\n",
    "\n",
    "x_historico<-data.frame(x=x_0[1],y=x_0[2])\n",
    "for (i in 1:1000){\n",
    "    g <- mygradient(x_0,h,myfuncVector)\n",
    "    if(abs(sum(g))<criterio_convergencia){\n",
    "        break;\n",
    "    }\n",
    "    x_new <- x_0 - g*delta     \n",
    "    x_0 <- x_new\n",
    "    x_historico<-rbind(x_historico,data.frame(x=x_0[1],y=x_0[2]))\n",
    "}\n",
    "nrow(x_historico)\n",
    "points(x_historico$x,x_historico$y,col=\"black\")\n",
    "color.scale.lines(x_historico$x,x_historico$y,c(1,1,0),0,c(0,1,1),colvar=1:nrow(x_historico),lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton\n",
    "\n",
    "El método de Newton es un método utilizado para encontrar, de forma iterativa, la raíz de una función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_x<-function(x){x^3}\n",
    "\n",
    "x<-seq(-9,3,length.out = 100)\n",
    "y<-f_x(x)\n",
    "\n",
    "options(repr.plot.height=4,repr.plot.width=6)\n",
    "plot(x,y,t=\"l\")\n",
    "abline(h=0)\n",
    "grid()\n",
    "options(repr.plot.height=5,repr.plot.width=8)\n",
    "\n",
    "\n",
    "plotDerivada<-function(x,color=\"red\"){\n",
    "    delta <- 3\n",
    "    lines(c(x-delta,x+delta),\n",
    "          c(f_x(x)-delta*3*x^2,f_x(x)+delta*3*x^2),\n",
    "          col=color)\n",
    "}\n",
    "#plotDerivada(-7)\n",
    "#abline(v=-4.65,col=\"gray\")\n",
    "#plotDerivada(-4.65,color=\"blue\")\n",
    "#abline(v=-3.1,col=\"gray\")\n",
    "#plotDerivada(-3.1,color=\"orange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método iterativo aproxima el valor de la función a 0 siguiendo los siguientes pasos:\n",
    "1. Selecciona un punto inicial: $x_0$\n",
    "1. Traza una línea tangente a la curva, es decir, con pendiente igual a la derivada.\n",
    "1. Proyectar la línea anterior hasta que corte en $y=0$. El valor de $x$ donde corta será el nuevo $x_0$\n",
    "1. Repetir desde el paso 2 hasta que $f(x_0)$ sea muy cercano a 0.\n",
    "\n",
    "Estos pasos se pueden simplificar de la siguiente forma:\n",
    "\n",
    "Podemos aproximar la functión $f(x)$ en un punto $x$ cercano a $x_0$ de la siguiente forma. Esto se considera una aproximación al desarrollo en serie de Taylor solo con la primera derivada:\n",
    "\\\\[\n",
    "f(x) \\approx  f(x_0)+(x-x_0)·f'(x_0)\n",
    "\\\\]\n",
    "\n",
    "Si igualamos $f(x)=0$ y despejamos $x$:\n",
    "\\\\[\n",
    "\\begin{split}\n",
    "0 &= f(x_0)+(x-x_0)·f'(x_0) \\\\\n",
    "0 &= f(x_0)+x·f'(x_0)-x_0·f'(x_0) \\\\\n",
    "x_0·f'(x_0) - f(x_0) &= x·f'(x_0) \\\\\n",
    "x_0 - \\frac{f(x_0)}{f'(x_0)} &= x \\\\\n",
    "\\end{split}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar el mínimo de una función equivale a encontrar la raiz de su derivada. Es decir, el mínimo siempre se encuentra en $f'(x)=0$. Por lo tanto podemos usar el método de Newton para encontrar el mínimo de una función. Solo hay que reemplazar $f(x)$ por $f'(x)$:\n",
    "\\\\[\n",
    "x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\n",
    "\\\\]\n",
    "Aplicamos de forma iterativa esa ecuación hasta que alcanzamos un mínimo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton en optimización\n",
    "\n",
    "Para poder usar el método de Newton en optimización tenemos que generalizarlo a un espacio vectorial. \n",
    "\n",
    "#### Matriz Jacobiana\n",
    "\n",
    "Es la generalización del concepto **derivada** aplicada a funciones de vectores. \n",
    "\n",
    "\n",
    "Supongamos que tenemos una función $f: \\mathbb {R}^n \\rightarrow  \\mathbb {R}$.\n",
    "En este caso el Jacobiano coincide con el gradiente: \n",
    "\\\\[\n",
    "{\\boldsymbol  \\nabla }f({\\vec {x}})={\\begin{bmatrix}{\\cfrac  {\\partial y}{\\partial x_{1}}}&\\ldots &{\\cfrac  {\\partial y}{\\partial x_{n}}}\\end{bmatrix}}\n",
    "\\\\]\n",
    "\n",
    "Si tenemos una función $f: \\mathbb {R}^n \\rightarrow  \\mathbb {R}^m$ el Jacobiano es la derivada parcial de todas las posibles combinaciones de coordenadas.\n",
    "\\\\[\n",
    "J_{\\mathbf {f} }(x_{1},\\ldots ,x_{n}) = {\\begin{bmatrix}{\\cfrac  {\\partial y_{1}}{\\partial x_{1}}}&\\cdots &{\\cfrac  {\\partial y_{1}}{\\partial x_{n}}}\\\\\\vdots &\\ddots &\\vdots \\\\{\\cfrac  {\\partial y_{m}}{\\partial x_{1}}}&\\cdots &{\\cfrac  {\\partial y_{m}}{\\partial x_{n}}}\\end{bmatrix}}\n",
    "\\\\]\n",
    "\n",
    "#### Matriz Hessiana\n",
    "\n",
    "Es la generalización del concepto **derivada segunda** aplicada a funciones de vectores. \n",
    "\n",
    "Dada una función $f: \\mathbb {R}^n \\rightarrow  \\mathbb {R}$, la segunda derivada respecto a $x_i$ y $x_j$ es \n",
    "${\\frac  {\\partial ^{2}}{\\partial x_{i}\\,\\partial x_{j}}}f \\left(\\vec{x}\\right)$. Es decir:\n",
    "\n",
    "\\\\[\n",
    "H_{{f}}\\left(\\vec{x}\\right)_{{i,j}}={\\frac  {\\partial ^{2}}{\\partial x_{i}\\,\\partial x_{j}}}f \\left(\\vec{x}\\right).\n",
    "\\\\]\n",
    "\n",
    "Esto se convierte en una matriz de la siguiente forma:\n",
    "\\\\[\n",
    "H(f)=\\begin{bmatrix}{\\frac  {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\frac  {\\partial ^{2}f}{\\partial x_{1}\\partial x_{2}}}&\\cdots &{\\frac  {\\partial ^{2}f}{\\partial x_{1}\\partial x_{n}}}\\\\{\\frac  {\\partial ^{2}f}{\\partial x_{2}\\partial x_{1}}}&{\\frac  {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\frac  {\\partial ^{2}f}{\\partial x_{2}\\partial x_{n}}}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\{\\frac  {\\partial ^{2}f}{\\partial x_{n}\\partial x_{1}}}&{\\frac  {\\partial ^{2}f}{\\partial x_{n}\\partial x_{2}}}&\\cdots &{\\frac  {\\partial ^{2}f}{\\partial x_{n}^{2}}}\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "La matriz **Hessiana** es el **Jacobiano** del gradiente.\n",
    "\n",
    "**Ejemplo**:\n",
    "\\\\[\n",
    "f(x_1,x_2)=x_1^2+x_2^3\n",
    "\\\\]\n",
    "Sabemos que:\n",
    "\\\\[\n",
    "\\begin{split}\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_1} &= 2·x_1 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_2} &= 3·x_2^2\n",
    "\\end{split}\n",
    "\\\\]\n",
    "\n",
    "Su matriz Hessiana se calcula como:\n",
    "\\\\[\n",
    "\\begin{split}\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_1 x_1} &= 2 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_1 x_2} &= 0 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_2 x_1} &= 0 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_2 x_2} &= 6·x_2 \n",
    "\\end{split}\n",
    "\\\\]\n",
    "\n",
    "El resultado es:\n",
    "\\\\[\n",
    "H(f_\\vec{x})=\\begin{bmatrix} \n",
    "2 & 0 \\\\\n",
    "0 & 6·x_2\n",
    "\\end{bmatrix}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton\n",
    "\n",
    "La ecuación:\n",
    "\\\\[\n",
    "x = x_0 - \\frac{f'(x_0)}{f''(x_0)}\n",
    "\\\\]\n",
    "la podemos generalizar reemplazando $f'(x)$ con el gradiente y $f''(x)$ con la matriz Hessiana.\n",
    "\n",
    "\\\\[\n",
    "\\vec x = \\vec x_0 - [\\mathbf {H} f\\left(\\vec {x}_0 \\right)]^{-1}·\\nabla f(\\mathbf {x}_0)\n",
    "\\\\]\n",
    "\n",
    "Podemos añadir una tasa de aprendizaje (learning rate) $\\epsilon$, un hiperparámetro, que hay que definir al principio de la iteración. Se le conoce como **tasa de aprendizaje** (learning rate). Un valor pequeño puede hacer que se necesiten muchas iteraciones para converger, una valor grande puede hacernos perder el máximo de la función.\n",
    "\n",
    "\n",
    "\\\\[\n",
    "\\vec x = \\vec x_0 - \\epsilon [\\mathbf {H} f\\left(\\vec {x}_0 \\right)]^{-1}·\\nabla f(\\mathbf {x}_0)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo en R**\n",
    "\n",
    "En R podemos usar el paquete *pracma* que tiene las funciones *grad()* y *hessian()* para calcular el gradiente y la matriz Hessiana.\n",
    "\n",
    "Por ejemplo, vamos a calcular en R el gradiente y la matriz Hessiana de $f(x_1,x_2)=x_1^2+x_2^3$, en el punto $[2,1]$ que sabemos que es:\n",
    "\\\\[\n",
    "\\nabla (f_\\vec{x})=\\begin{bmatrix} \n",
    "2·x_1 & 3·x_2^2 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "4 & 3 \\\\\n",
    "\\end{bmatrix} \n",
    "\\\\]\n",
    "\\\\[\n",
    "H(f_\\vec{x})=\\begin{bmatrix} \n",
    "2 & 0 \\\\\n",
    "0 & 6·x_2\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} \n",
    "2 & 0 \\\\\n",
    "0 & 6\n",
    "\\end{bmatrix}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pracma)\n",
    "f_x1x2<-function(x){x[1]^2+x[2]^3}\n",
    "xvec<-c(2,1)\n",
    "print(\"El gradiente es:\")\n",
    "grad(f_x1x2,xvec)\n",
    "print(\"La matriz Hessiana es:\")\n",
    "hessian(f_x1x2,xvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pracma)\n",
    "library(plotrix)\n",
    "myfunc3D<-function(x1,x2){0.2*x1^4+x2^2}\n",
    "#myfunc3D<-function(x1,x2){x1^2+3*x2^2}\n",
    "\n",
    "myfuncVector<-function(x){ myfunc3D(x[1],x[2])  }\n",
    "\n",
    "mygradient<-function(x,h,func){ \n",
    "  c((func(c(x[1]+h,x[2]))-func(x))/h,\n",
    "    (func(c(x[1],x[2]+h))-func(x))/h) }\n",
    "\n",
    "x_0 <- c(-9,-9)\n",
    "delta<- 0.025 \n",
    "#delta <- 1 \n",
    "\n",
    "h <- 0.001\n",
    "criterio_convergencia<-0.05\n",
    "\n",
    "\n",
    "x_historico<-data.frame(x=x_0[1],y=x_0[2])\n",
    "for (i in 1:100){\n",
    "  g <- mygradient(x_0,h,myfuncVector)\n",
    "  #g <- inv(hessian(myfuncVector,x_0)) %*% as.matrix(grad(myfuncVector,x_0),cols=1)\n",
    "  x_new <- x_0 - g*delta\n",
    "  if(abs(sum(g))<criterio_convergencia){\n",
    "    break;\n",
    "  }       \n",
    "  x_0 <- x_new\n",
    "  x_historico<-rbind(x_historico,data.frame(x=x_0[1],y=x_0[2]))\n",
    "}\n",
    "nrow(x_historico)\n",
    "\n",
    "x1 <- seq(-10,10,length=100)\n",
    "x2 <- seq(-10,10,length=100)\n",
    "z <- outer(x1,x2,myfunc3D)\n",
    "image(x1,x2,z,col = terrain.colors(39))\n",
    "contour(x1,x2,z,add=T)\n",
    "points(x_historico$x,x_historico$y,col=\"black\")\n",
    "color.scale.lines(x_historico$x,x_historico$y,c(1,1,0),0,c(0,1,1),colvar=1:nrow(x_historico),lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problemas del método de Newton\n",
    "\n",
    "* El calcular la matriz Hessiana es costoso computacionalmente hablando.\n",
    "* Cuando la función es convexa y cuadrática converge en un solo salto, si no es cuadrática hacen falta más iteraciones.\n",
    "* Es útil solo cuando el punto crítico cercano es un mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pracma)\n",
    "library(plotrix)\n",
    "myfunc3D<-function(x1,x2){sin(x1)*sin(x2)/(x1*x2)}\n",
    "x <- seq(-10,10,length=100)\n",
    "y <- seq(-10,10,length=100)\n",
    "z <- outer(x,y,myfunc3D)\n",
    "image(x,y,z,col = terrain.colors(39))\n",
    "contour(x,y,z,add=T)\n",
    "\n",
    "myfuncVector<-function(x){ myfunc3D(x[1],x[2]) }\n",
    "\n",
    "\n",
    "x_0 <- c(-2,-7)\n",
    "x_0 <- c(-5,-1)\n",
    "#x_0 <- c(-5,-4)\n",
    "#x_0 <- c(0.7,1)\n",
    "delta <- 0.5\n",
    "criterio_convergencia<-1e-4\n",
    "\n",
    "x_historico<-data.frame(x=x_0[1],y=x_0[2])\n",
    "for (i in 1:1000){\n",
    "    g <- grad(myfuncVector,x_0)\n",
    "    g <- inv(hessian(myfuncVector,x_0)) %*% as.matrix(grad(myfuncVector,x_0),cols=1)\n",
    "    if(abs(sum(g))<criterio_convergencia){\n",
    "        break;\n",
    "    }\n",
    "    x_new <- x_0 - g*delta     \n",
    "    x_0 <- x_new\n",
    "    x_historico<-rbind(x_historico,data.frame(x=x_0[1],y=x_0[2]))\n",
    "}\n",
    "paste(\"Numero pasos\",nrow(x_historico))\n",
    "paste(\"Valor alcanzado\",myfuncVector(x_0))\n",
    "points(x_historico$x,x_historico$y,col=\"black\")\n",
    "color.scale.lines(x_historico$x,x_historico$y,c(1,1,0),0,c(0,1,1),colvar=1:nrow(x_historico),lwd=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones en R para optimización\n",
    "\n",
    "Podemos utilizar diferentes funciones en R que, dada una función nos encuentran el mínimo iterando de forma similar a como hemos visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc2<-function(x){0.05*x^2+sin(x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o<-optimize(myfunc2,interval=c(-10,10))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-seq(-10,10,length.out = 100)\n",
    "y<-myfunc2(x)\n",
    "\n",
    "options(repr.plot.height=4,repr.plot.width=6)\n",
    "plot(x,y,t=\"l\")\n",
    "points(o$minimum, o$objective,col=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myfunc3D<-function(x1,x2){sin(x1+0.01)*sin(x2)/(x1*(x2-0.005))}\n",
    "myfunc3D<-function(x1,x2){log(abs((sin(x1)*(x2*x1+3))^2*cos(x2)))}\n",
    "\n",
    "plotMyFunc<-function(myfunc,margin){\n",
    "    x <- seq(margin[1],margin[2],length=100)\n",
    "    y <- seq(margin[1],margin[2],length=100)\n",
    "    z <- outer(x,y,myfunc)\n",
    "    image(x,y,z,col = terrain.colors(39))\n",
    "    contour(x,y,z,add=T)\n",
    "}\n",
    "    \n",
    "myfuncVector<-function(x){ myfunc3D(x[1],x[2]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o<-optim(c(5,1), myfuncVector,hessian=TRUE)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMyFunc(myfunc3D,c(-2,7))\n",
    "points(o$par[1],o$par[2],col=\"blue\",c=2,pch='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o<-optim(c(-5,1), myfuncVector,hessian=TRUE)\n",
    "plotMyFunc(myfunc3D,c(-6,2))\n",
    "points(o$par[1],o$par[2],col=\"blue\",c=2,pch='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programación no lineal\n",
    "\n",
    "En matemáticas, [programación no lineal](https://es.wikipedia.org/wiki/Programaci%C3%B3n_no_lineal) (PNL) es el proceso de resolución de un sistema de igualdades y desigualdades sujetas a un conjunto de restricciones sobre un conjunto de variables reales desconocidas, con una función objetivo a maximizar (o minimizar), cuando alguna de las restricciones o la función objetivo no son lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica del multiplicador de Lagrange te permite encontrar el máximo o mínimo de una función multivariable $f (x, y, \\dots)$ cuando hay alguna restricción en los valores de entrada que puede usar.\n",
    "\n",
    "Esta técnica solo se aplica a las restricciones del tipo: $g (x, y, \\dots) = c$\n",
    "\n",
    "Más información:\n",
    "\n",
    "https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Calcular el mínimo de:\n",
    "\\\\[\n",
    "  f(x,y)=x^2+y+3\n",
    "\\\\]\n",
    "\n",
    "Condicionado a:\n",
    "\\\\[\n",
    "  g(x,y)=x+y=5\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f<-function(x,y){\n",
    "    x^2+y+3\n",
    "}\n",
    "g<-function(x,y){\n",
    "    x+y\n",
    "}\n",
    "const <- 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra condición:\n",
    "\\\\[\n",
    "  g(x,y)=x+y=5\n",
    "\\\\]\n",
    "se puede reescribir como:\n",
    "\\\\[\n",
    "  y=5-x\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMyFunc(f,c(-5,7))\n",
    "abline(c(const,-1),col=\"red\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inv<-function(f,x){\n",
    "    y<- f-x^2-3\n",
    "    return(y)\n",
    "}\n",
    "g_inv<-function(g,x){\n",
    "    g-x\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-seq(-2,5,length.out=100)\n",
    "y<-g_inv(const,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf<-f_inv(8,x)\n",
    "\n",
    "plot(x,y,t='l',col='red')\n",
    "lines(x,yf,t='l',col='blue')\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0.5,4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn<-function(coefs){ f(coefs[1],coefs[2])}\n",
    "gn<-function(coefs){ g(coefs[1],coefs[2])}\n",
    "\n",
    "Rsolnp::solnp(c(3,2), \n",
    "              fun = fn, \n",
    "              eqfun = gn, \n",
    "              eqB = const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Podemos hacer una campaña de marketing en dos medios, dos emisoras de radios diferentes:\n",
    "\n",
    "- En la radio 1 tienen la siguiente formula de facturación por minutos t\n",
    "\n",
    " precio en euros = 0.5·t1 + t1^2\n",
    "\n",
    "\n",
    "- En la radio 2 tienen la siguiente formula de facturación por minutos t\n",
    "\n",
    " precio en euros = 3·t2 + 0.5·t2^2\n",
    "\n",
    "Así que el precio total será la suma de ambos tiempos\n",
    "\\\\[\n",
    "  f(t1,t2) = 0.5·t1+t1^2 + 3·t2+0.5·t2^2\n",
    "\\\\]\n",
    "\n",
    "Queremos que en total se oiga durante 60 minutos\n",
    "\\\\[\n",
    "  g(t1,t2)=t1+t2=60\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn<-function(t){\n",
    "    0.5*t[1]+t[1]^2+3*t[2]+0.5*t[2]^2\n",
    "}\n",
    "fn(c(60,0))\n",
    "fn(c(0,60))\n",
    "fn(c(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn<-function(t){\n",
    "    t[1]+t[2]\n",
    "}\n",
    "const <- 60\n",
    "\n",
    "out <- Rsolnp::solnp(c(30,30), \n",
    "              fun = fn, \n",
    "              eqfun = gn, \n",
    "              eqB = const)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paste(\"Tendríamos que emitir \",round(out$par[1],2),\" en la emisora 1  y \",round(out$par[2],2), \"en la emisora 2\")\n",
    "paste(\"El coste óptimo (mínimo) sería de\",round(fn(out$par),3),\"euros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Cartera de fondos de inversión minimizando la varianza\n",
    "\n",
    "Partimos de varios fondos de inversión y queremos una cartera que nos maximice el retorno medio a un año mientras minimiza la varianza.\n",
    "\n",
    "Queremos calcular unos coeficientes que nos digan que porcentaje de nuestra cartera corresponde a cada fondo.\n",
    "\n",
    "\\\\[\n",
    "portfolio=\\sum_i c_i · fondo_i\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la cartera de fondos de inversión con la rentabilidad a un año vista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds<-read.csv('data/fondos_inversion.csv')\n",
    "funds$date<-as.Date(funds$date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "ggplot(funds,aes(x=date))+geom_line(aes(y=fondo.1,color=\"fondo1\"))+geom_line(aes(y=fondo.2,color=\"fondo2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset<-funds\n",
    "df_subset$date<-NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos optimizar un retorno máximo y una varianza mínima de nuestro portfolio:\n",
    "\\\\[\n",
    "   portfolio=\\sum_i c_i · fondo_i\n",
    "\\\\]\n",
    "\n",
    "Dadas las variables:\n",
    "\\\\[\n",
    "    mn=\\frac{1}{num days} \\sum_{date} portfolio \\\\\n",
    "    vr= VAR \\left( portfolio \\right) \\\\\n",
    "\\\\]\n",
    "\n",
    "Queremos encontrar los coeficientes $c_i$ tal que la siguiente función sea mínima:\n",
    "\\\\[\n",
    "    f(c)=\\frac{-mn}{exp(1+vr*100)}\n",
    "\\\\]\n",
    "\n",
    "Cumpliendo las siguientes limitaciones:\n",
    "\\\\[\n",
    "\\sum c_i = 1 \\\\\n",
    "0 \\leq c_i \\leq 1\n",
    "\\\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pracma)\n",
    "variance_importance <- 100\n",
    "\n",
    "roi_optim<-function(coefs){\n",
    "    mn<-mean(rowSums(repmat(coefs,nrow(df_subset),1) * df_subset))    \n",
    "    vr<-var(rowSums(repmat(coefs,nrow(df_subset),1) * df_subset))\n",
    "    return (-mn/exp(1+variance_importance*vr))\n",
    "    #return(mn)\n",
    "}\n",
    "constrain_func<-function(coefs){\n",
    "    sum(coefs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x0<-rep(1/ncol(df_subset),ncol(df_subset)) #Valor inicial de los coeficientes\n",
    "lx <- rep(0,length(x0)) # Valor mínimo de los coeficientes\n",
    "ux <- rep(1,length(x0)) # Valor máximo de los coeficientes\n",
    "sol1 <- Rsolnp::solnp(x0, fun = roi_optim, eqfun = constrain_func, eqB = 1, LB=lx, UB=ux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs<-sol1$pars    \n",
    "names(coefs)<-colnames(df_subset)\n",
    "coefs<-round(coefs[coefs>1e-2],4)\n",
    "coefs<-coefs/sum(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular el fondo agregado resultante sería:\n",
    "\n",
    "\\\\[\n",
    "\\begin{bmatrix}\n",
    " x_{11} & x_{12} & x_{13} & x_{14} \\\\ \n",
    " x_{21} & x_{22} & x_{23} & x_{24} \\\\ \n",
    " \\vdots & & & \\ddots & \\vdots \\\\ \n",
    " x_{n1} & x_{n2} & x_{n3} & x_{n4}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "coef_1 \\\\ \n",
    "coef_2 \\\\\n",
    "coef_3 \\\\\n",
    "coef_4 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " y_1 \\\\ \n",
    " y_2 \\\\ \n",
    " \\vdots \\\\ \n",
    " y_n \n",
    "\\end{bmatrix}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds[\"portfolio\"]<-as.matrix(funds[names(coefs)])%*%coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(funds,aes(x=date))+geom_line(aes(y=portfolio,color=\"minima varianza\"))+\n",
    " geom_line(aes(y=fondo.10,color=\"fondo.10\"))+\n",
    "geom_line(aes(y=fondo.15,color=\"fondo.15\"))+\n",
    "geom_line(aes(y=fondo.19,color=\"fondo.19\"))+\n",
    "geom_line(aes(y=fondo.20,color=\"fondo.20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10^(colMeans(log10(funds[c(\"portfolio\",names(coefs))]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(funds[c(\"portfolio\",names(coefs))],2,var )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
