{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización\n",
    "\n",
    "Si el modelo es demasiado complejo ocurre el **sobreajuste** el modelo aprende sobre el ruido de nuestro modelo de entrenamiento y no es capaz de  generalizar bien.\n",
    "Para evitar el sobreajuste(overfitting) se puede recurrir a simplificar el modelo o a poner limitaciones sobre el mismo. Esto se conoce con el nombre de regularización.\n",
    "* Regularización Lasso o $L$: permite seleccionar los parámetros que más afectan al resultado. Se añade la función de coste:\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+\\lambda \\sum_j | \\beta |\n",
    "\\\\]\n",
    "* Regularización Ridge o $L^2$: se evita que los parámetros crezcan demasiado. Se añade la función de coste:\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+\\lambda \\sum_j \\beta^2\n",
    "\\\\]\n",
    "* Elástica: Una solución de compromiso entre las dos:\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+ \\alpha \\lambda \\sum_j | \\beta |+(1-\\alpha)/2 \\lambda \\sum_j \\beta^2\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo regularización\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de regresión lineal con tres variables: x0, x1 y x2.\n",
    "\n",
    "La función que tratamos de modelizar es del tipo:\n",
    "\\\\[\n",
    "y=\\beta_1+\\beta_2·x_1+\\beta_3·x_2+\\beta_4·x_3\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función que queremos modelizar en función de beta\n",
    "myfunction<-function(x_,beta){beta[1]+x_[,1]*beta[2]+x_[,2]*beta[3]+x_[,3]*beta[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_real<-c(3,0.1,-5,4)\n",
    "\n",
    "set.seed(123)\n",
    "get_example_data_frame<-function(n=100){    \n",
    "    x1<-runif(n,min=-10,max=10)\n",
    "    x2<-rnorm(n,mean=7,sd=9)\n",
    "    x3<-x1*rnorm(n,mean=3,sd=2)\n",
    " \n",
    "    df<-data.frame(x1,x2,x3)\n",
    "    df$y=myfunction(df,beta=beta_real)+rnorm(n,mean=0,sd=10)\n",
    "    df\n",
    "}\n",
    "df<-get_example_data_frame()\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(GGally)\n",
    "options(repr.plot.height=4,repr.plot.width=6,repr.plot.res = 200)\n",
    "ggpairs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- lm(y~x1+x2+x3,data=df)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pracma)\n",
    "#Definimos una función de optimización que nos permite ver como evoluciona Beta\n",
    "\n",
    "optim_beta<-function(mymodel,mse,maxiter=1e3,delta=0.0002,beta_0=c(0,0,0,0)){\n",
    "    \n",
    "    x_historico<-data.frame(beta1=rep(NA,maxiter),\n",
    "                            beta2=rep(NA,maxiter),\n",
    "                            beta3=rep(NA,maxiter),\n",
    "                            beta4=rep(NA,maxiter),\n",
    "                            mse=rep(NA,maxiter))\n",
    "    x_historico[1,]<-c(beta_0,mse(beta_0))\n",
    "\n",
    "    for (i in 2:maxiter){\n",
    "        g <- grad(mse,beta_0)        \n",
    "        beta_new <- beta_0 - g*delta\n",
    "        beta_0 <- beta_new\n",
    "        x_historico[i,]<-c(beta_0,mse=mse(beta_0))\n",
    "    }\n",
    "    x_historico<-na.omit(x_historico)\n",
    "    nrow(x_historico)\n",
    "    x_historico$step=1:nrow(x_historico)\n",
    "    x_historico\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la métrica que queremos minimizar, en este caso el error cuadrático medio\n",
    "mse<-function(beta){\n",
    "    sum((df$y-myfunction(df[,c('x1','x2','x3')],beta))^2)/nrow(df)    \n",
    "}\n",
    "x_historico<-optim_beta(myfunction,mse,maxiter=1e4,delta=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=4,repr.plot.width=8)\n",
    "g1<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta1),color='red')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[1],color='black')\n",
    "g2<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta2),color='blue')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[2],color='black')\n",
    "g3<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta3),color='green')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[3],color='black')\n",
    "g4<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta4),color='orange')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[4],color='black')\n",
    "\n",
    "library(egg)\n",
    "grid.arrange(g1, g2, g3,g4, nrow = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularización Lasso\n",
    "\n",
    "Ahora vamos a aplicar una regularización L1 o de Lasso, añadamos el término:\n",
    "\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+\\lambda \\sum_j | \\beta_j |\n",
    "\\\\]\n",
    "\n",
    "De los coeficientes $\\beta_j$ no tenemos en cuenta la intersección. Notar que:\n",
    "* Cuando $\\lambda \\rightarrow 0$, entonces $\\beta_{LASSO} \\rightarrow \\beta_{original mínimos cuadrados}$ \n",
    "* Cuando $\\lambda \\rightarrow \\infty$, entonces $\\beta_{LASSO} \\rightarrow 0$ \n",
    "\n",
    "Tratamos de minimizar la suma de todos los demás coeficientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda<-40\n",
    "lambda<-2\n",
    "loss_lasso<-function(beta){\n",
    "    sum((df$y-myfunction(df[,-ncol(df)],beta))^2)/(2*nrow(df))+lambda*sum(abs(beta[-1]))\n",
    "}\n",
    "x_historico<-optim_beta(myfunction,loss_lasso,maxiter=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=4,repr.plot.width=8)\n",
    "library(ggplot2)\n",
    "\n",
    "g1<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta1),color='red')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[1],color='black')\n",
    "g2<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta2),color='blue')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[2],color='black')\n",
    "g3<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta3),color='green')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[3],color='black')\n",
    "g4<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta4),color='orange')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[4],color='black')\n",
    "\n",
    "library(egg)\n",
    "grid.arrange(g1, g2, g3,g4, nrow = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regularización **Lasso** descarta los componentes que tienen menos certeza en el resultado final, es decir, los coeficientes con un pvalor más alto.\n",
    "\n",
    "En R existe el paquete [glmnet](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) que te permite apilcar regularización a modelos lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "model<-glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),lambda=2,alpha=1, standardize=F)\n",
    "coefficients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que nos decantamos por una regularización Lasso ¿Cómo obtenemos el valor óptimo de $\\lambda$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit<-glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),alpha=1, standardize=F)\n",
    "par(mfrow=c(1,3))\n",
    "plot(fit,label=TRUE)\n",
    "grid()\n",
    "plot(fit,label=TRUE,xvar=\"lambda\")\n",
    "grid()\n",
    "plot(fit,label=TRUE,xvar=\"dev\")\n",
    "grid()\n",
    "#print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos los coeficientes para un valor dado de lambda\n",
    "l<-exp(2)\n",
    "coef(fit,s=l)\n",
    "#Mostramos el valor de R^2 que se corresponde con la Fracción de la desviación típica explicada\n",
    "caret::postResample(predict(fit,newx=as.matrix(df[,c('x1','x2','x3')]),s=l),df$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ir probando con varios valores de $\\lambda$ hasta encontrar con el *óptimo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 <- 2\n",
    "lambda2 <- round(exp(6))\n",
    "df_test<-get_example_data_frame(n=100)\n",
    "df_test[,c(\"pred1\",\"pred2\")]<-predict(fit,newx=as.matrix(df_test[,c('x1','x2','x3')]),s=c(lambda1,lambda2))\n",
    "head(df_test)\n",
    "ggplot(df_test,aes(x=y))+\n",
    "    geom_point(aes(y=pred1,color=paste0(\"lambda=\",lambda1)))+\n",
    "    geom_point(aes(y=pred2,color=paste0(\"lambda=\",lambda2)))+\n",
    "    geom_abline(slope=1,intercept=0,color=\"gray\")+\n",
    "    scale_colour_discrete(name = \"Valor lambda\" )+\n",
    "    ylab(\"prediction\")+xlab(\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busca el mejor elemento utilizando validación cruzada (*cross-validation*):\n",
    "\n",
    "![](img/cross-validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit<-cv.glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),nfolds=10,alpha=1, standarize=F)\n",
    "plot(cvfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo que tiene un error cuadrático medio más bajo aparece en la variable *cvfit$lambda.min*\n",
    "\n",
    "El modelo que tiene un mayor valor de $\\lambda$ cuya varianza del error está dentro de 1 *desviación típica* del *mínimo* aparece en la variable *cvfit$lambda.1se*. \n",
    "\n",
    "Al hacer cross-validation el MSE no será un valor único sino que tendremos *nfolds* diferentes. De todos estos MSE podemos calular la *media* y la *desviación típica*. El valor de *lambda.1se* viene a significar como el modelo más sencillo ($\\lambda$ más alto) que no se diferencia considerablemente del *mínimo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=2,repr.plot.width=5)\n",
    "lamda_se<-1\n",
    "qsd009<-function(x){    \n",
    "    out<-dnorm(x)\n",
    "    out[x> lamda_se  | x< -lamda_se  ]<-NA\n",
    "    out\n",
    "}\n",
    "xdf<-data.frame(z=c(-4,4))\n",
    "ggplot(xdf,aes(x=z))+stat_function(fun=dnorm)+\n",
    "  stat_function(fun=qsd009, geom=\"area\",fill=\"red\")+\n",
    "  geom_text(x=0,y=0.2,size=4,label=paste0(100*round(pnorm(lamda_se)-pnorm(-lamda_se),4),\"%\"))+  \n",
    "  theme_linedraw()\n",
    "options(repr.plot.height=7,repr.plot.width=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit$lambda.min\n",
    "cvfit$lambda.1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(cvfit, s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su R^2 estimado será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit$glmnet.fit$dev.ratio[which(cvfit$glmnet.fit$lambda == cvfit$lambda.1se)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, para hacer una predicción Lasso con glmnet haríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cvfit<-cv.glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),nfolds=10,alpha=1, standarize=F)\n",
    "\n",
    "df_test<-get_example_data_frame(n=100)\n",
    "df_test[,c(\"pred\")]<-predict(cvfit,newx=as.matrix(df_test[,c('x1','x2','x3')]),s=cvfit$lambda.1se)\n",
    "head(df_test)\n",
    "caret::postResample(df_test$y,df_test$pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularización Ridge\n",
    "\n",
    "Ahora vamos a aplicar una regularización L2 o Ridge, añadamos el término:\n",
    "\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+\\lambda \\sum_j \\beta_j^2\n",
    "\\\\]\n",
    "\n",
    "De los coeficientes $\\beta_j$ no tenemos en cuenta la intersección. Notar que:\n",
    "* Cuando $\\lambda \\rightarrow 0$, entonces $\\beta_{RIDGE} \\rightarrow \\beta_{original mínimos cuadrados}$ \n",
    "* Cuando $\\lambda \\rightarrow \\infty$, entonces $\\beta_{RIDGE} \\rightarrow 0$ \n",
    "\n",
    "Tratamos de minimizar la suma de todos los demás coeficientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda<-10\n",
    "#lambda<-1000\n",
    "loss_ridge<-function(beta){\n",
    "    mean((df$y-myfunction(df[,c('x1','x2','x3')],beta))^2)+lambda*sum(beta[c(2,3,4)]^2)\n",
    "}\n",
    "\n",
    "x_historico<-optim_beta(myfunction,loss_ridge,maxiter=1e4,delta=0.0005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=4,repr.plot.width=8)\n",
    "library(ggplot2)\n",
    "\n",
    "g1<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta1),color='red')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[1],color='black')\n",
    "g2<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta2),color='blue')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[2],color='black')\n",
    "g3<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta3),color='green')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[3],color='black')\n",
    "g4<-ggplot(x_historico,aes(x=step))+geom_line(aes(y=beta4),color='orange')+\n",
    "    theme(axis.text.x = element_text(angle = 45))+geom_hline(yintercept=beta_real[4],color='black')\n",
    "\n",
    "library(egg)\n",
    "grid.arrange(g1, g2, g3,g4, nrow = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regularización **Ridge** acerca los coeficientes de las variables más correladas el uno hacia al otro.\n",
    "\n",
    "También podemos usar el paquete [glmnet](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html), en este caso con $\\alpha=0$ ya que utiliza la siguiente función de coste:\n",
    "\\\\[\n",
    "Coste = {1 \\over n} \\sum_{i=0}^n{(Y-\\hat{Y})^2}+ \\alpha \\lambda \\sum_j | \\beta_j |+(1-\\alpha)/2 \\lambda \\sum_j \\beta_j^2\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "model<-glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),lambda=20,alpha=0, standardize=F)\n",
    "coefficients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit<-glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),alpha=0, standardize=F)\n",
    "\n",
    "par(mfrow=c(1,3))\n",
    "plot(fit,label=TRUE)\n",
    "plot(fit,label=TRUE,xvar=\"lambda\")\n",
    "plot(fit,label=TRUE,xvar=\"dev\")\n",
    "grid()\n",
    "#print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit<-cv.glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),nfolds=10,alpha=0, standardize=F)\n",
    "plot(cvfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo que tiene un error cuadrático medio más bajo aparece en la variable *cvfit$lambda.min*\n",
    "\n",
    "El modelo que tiene un mayor valor de $\\lambda$ cuya varianza del error está dentro de 1 *desviación típica* del *mínimo* aparece en la variable *cvfit$lambda.1se*. \n",
    "\n",
    "Al hacer cross-validation el MSE no será un valor único sino que tendremos *nfolds* diferentes. De todos estos MSE podemos calular la *media* y la *desviación típica*. El valor de *lambda.1se* viene a significar como el modelo más sencillo ($\\lambda$ más alto) que no se diferencia considerablemente del *mínimo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit$lambda.min\n",
    "cvfit$lambda.1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(cvfit, s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfit$glmnet.fit$dev.ratio[which(cvfit$glmnet.fit$lambda == cvfit$lambda.1se)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, para hacer una predicción Ridge con glmnet haríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cvfit<-cv.glmnet(as.matrix(df[,c('x1','x2','x3')]),as.matrix(df[,'y']),nfolds=10,alpha=0, standardize=F)\n",
    "\n",
    "df_test<-get_example_data_frame(n=100)\n",
    "df_test[,c(\"pred\")]<-predict(cvfit,newx=as.matrix(df_test[,c('x1','x2','x3')]),s=cvfit$lambda.1se)\n",
    "head(df_test)\n",
    "caret::postResample(df_test$y,df_test$pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciendo la dureza del hormigón con regularización\n",
    "\n",
    "Resumen: El hormigón es el material más importante en la ingeniería civil. La resistencia a la compresión del hormigón es una función altamente no lineal de la edad y ingredientes Estos ingredientes incluyen cemento, escoria de alto horno, cenizas volantes, agua, superplastificante, agregado grueso y agregado fino.\n",
    "\n",
    "Fuente:\n",
    "https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete<-read.csv(\"data/Concrete_Data.csv\",\n",
    "                   col.names=c(\"cemento\",\"escoria\",\"cenizas\",\"agua\",\"plastificante\",\"aggrueso\",\"agfino\",\"edad\",\"resistencia\"))\n",
    "head(concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=5,repr.plot.width=8,repr.plot.res = 200)\n",
    "\n",
    "GGally::ggpairs(concrete, \n",
    "       #lower = list(continuous = GGally::wrap(\"density\", alpha = 0.8,size=0.2,color='blue'))\n",
    "       lower = list(continuous = GGally::wrap(\"points\", alpha = 0.3,size=0.1,color='blue'))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(tidyverse))\n",
    "suppressPackageStartupMessages(library(glmnetUtils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(12)\n",
    "\n",
    "idx<-sample(1:nrow(concrete),nrow(concrete)*0.7)\n",
    "\n",
    "concrete.train=concrete[idx,]\n",
    "concrete.test=concrete[-idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model<-lm(resistencia~\n",
    "           cemento*escoria*agua*plastificante*aggrueso*agfino*edad*cenizas,data = concrete.train)\n",
    "    \n",
    "lm_yp_train<-predict(lm_model,concrete.train)\n",
    "caret::postResample(pred=lm_yp_train, obs=concrete.train$resistencia)\n",
    "\n",
    "\n",
    "lm_yp_test<-predict(lm_model,concrete.test)\n",
    "caret::postResample(pred=lm_yp_test, obs=concrete.test$resistencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularización**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso tenemos variables que provienen de diferentes unidades y tienen rangos muy diferentes de valores, por lo tanto para hacer los coeficientes comparables entre si, deberíamos estandarizarlos.\n",
    "\n",
    "El resultado consiste en dejar nuestros datos con media 0 y varianza 1:\n",
    "\\\\[\n",
    "X_n=\\frac{X-\\mu}{\\sigma}\n",
    "\\\\]\n",
    "\n",
    "esto lo realiza por si solo el paquete glmnet o glmnetutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv<-glmnetUtils::cv.glmnet(formula=resistencia~cemento*escoria*cenizas*agua*plastificante*aggrueso*agfino*edad,\n",
    "                            data=concrete.train,alpha=1,\n",
    "                            nfold= 10,\n",
    "                            type.measure=\"mse\",\n",
    "                            standardize = T)\n",
    "ggplot(data.frame(lambda=cv$lambda,cross_validated_mean_error=cv$cvm),\n",
    "       aes(x=lambda,y=cross_validated_mean_error))+geom_line()\n",
    "paste0(\"El valor lambda con el menor error es:\",cv$lambda.min)\n",
    "paste0(\"El valor lambda más alto que se encuentra a una distancia 1sd es:\",cv$lambda.1se)\n",
    "paste0(\"El R^2 estimado es\", cv$glmnet.fit$dev.ratio[which(cv$glmnet.fit$lambda == cv$lambda.1se)]) \n",
    "ggplot(data.frame(lambda=cv$lambda,r2=cv$glmnet.fit$dev.ratio),\n",
    "       aes(x=lambda,y=r2))+geom_line()+xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_yp_train <- predict(cv,\n",
    "                     concrete.train, \n",
    "                     s=cv$lambda.min)\n",
    "caret::postResample(reg_yp_train,concrete.train$resistencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_yp_test <- predict(cv,\n",
    "                     concrete.test, \n",
    "                     s=cv$lambda.min)\n",
    "caret::postResample(reg_yp_test,concrete.test$resistencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[\n",
    "MSE = {1 \\over n} \\sum_{i=0}^n{(y_i-y_i')^2}\n",
    "\\\\]\n",
    "\\\\[\n",
    "R^2=1-\\frac{\\sum_i (y_i-y_i')^2}{\\sum_i (y_i-\\bar{y})^2}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_test <- concrete.test$resistencia-reg_yp_test\n",
    "plot(concrete.test$resistencia,residual_test)\n",
    "qqnorm(residual_test)\n",
    "qqline(residual_test,col=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(real=concrete.test$resistencia,\n",
    "                       lm=lm_yp_test,\n",
    "                       reg=reg_yp_test[,1]) %>% \n",
    "   mutate(res_lm=real-lm) %>% \n",
    "   mutate(res_reg=real-reg) -> total_pred\n",
    "head(total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "\n",
    "total_pred %>% select(res_lm,res_reg) %>% melt() %>% \n",
    "  ggplot(aes(x=value,color=variable))+geom_density()\n",
    "\n",
    "\n",
    "total_pred %>% ggplot(aes(x=real))+\n",
    "  geom_point(aes(y=reg,color=\"Reg\"),alpha=0.5)+\n",
    "  geom_point(aes(y=lm,color=\"Lm\"),alpha=0.5)+geom_abline(intercept = 0,slope=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si utilizamos lambda.1se como valor de lambda** el R² y el MSE empeoran ligeramente, pero siguen siendo mejores que la regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_yp_train <- predict(cv,\n",
    "                     concrete.train, \n",
    "                     s=cv$lambda.1se)\n",
    "caret::postResample(reg_yp_train,concrete.train$resistencia)\n",
    "\n",
    "reg_yp_test <- predict(cv,\n",
    "                     concrete.test, \n",
    "                     s=cv$lambda.1se)\n",
    "caret::postResample(reg_yp_test,concrete.test$resistencia)\n",
    "\n",
    "data.frame(real=concrete.test$resistencia,\n",
    "                       lm=lm_yp_test,\n",
    "                       reg=reg_yp_test[,1]) %>% \n",
    "   mutate(res_lm=real-lm) %>% \n",
    "   mutate(res_reg=real-reg) -> total_pred\n",
    "\n",
    "total_pred %>% select(res_lm,res_reg) %>% melt() %>% \n",
    "  ggplot(aes(x=value,color=variable))+geom_density()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Tiene sesgo nuestro modelo?\n",
    "colMeans(total_pred[,c(\"res_lm\",\"res_reg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
